# -*- coding: utf-8 -*-
"""qlora_llama_example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FbeB79ytd7wvsjujPQiDWnPpSI1pL-Wb
"""

#!pip install transformers datasets peft bitsandbytes evaluate accelerate trl

from dataclasses import dataclass, field
import os

try:
  os.system("pip install flash-attn --no-build-isolation --upgrade")
except:
  print("flash-attn failed to install")

from typing import Optional
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    default_data_collator,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    HfArgumentParser,
    set_seed
)
import torch
from datasets import load_from_disk

import bitsandbytes as bnb
from huggingface_hub import login

from peft import (
    get_peft_model,
    LoraConfig,
    TaskType,
    prepare_model_for_kbit_training
)
from peft import AutoPeftModelForCausalLM
from peft.tuners.lora import LoraLayer

def find_all_linear_names(model: torch.nn.Module):
  lora_module_names = set()
  for name, module in model.named_modules():
    if isinstance(module, bnb.nn.Linear4bit):
      names = name.split('.')
      lora_module_names.add(names[0] if len(names) == 1 else names[-1])

  if "lm_head" in lora_module_names:
    lora_module_names.remove("lm_head")

  return list(lora_module_names)

def create_peft_model(model: torch.nn.Module, gradient_checkpointing=True, bf16=True):
  #Enabling qlora int-4 memory efficient training
  model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=gradient_checkpointing)

  if gradient_checkpointing:
    model.gradient_checkpointing_enable()

  modules = find_all_linear_names(model)
  print(f"Found {len(modules)} modules to quantize {modules}")

  lora_config = LoraConfig(
      r=64,
      lora_alpha=16,
      lora_dropout=0.1,
      target_modules=modules,
      bias="none",
      task_type=TaskType.CAUSAL_LM
  )
  model = get_peft_model(model, lora_config, adapter_name="default")

  # pre-process the model by upcasting the layer norms in float 32 for
  for name, module in model.named_modules():
      if isinstance(module, LoraLayer):
        if bf16:
          module = module.to(torch.bfloat16)

      # Low precision (such as bfloat16 or float16) in LayerNorm or BatchNorm can induce numerical instability
      # Both LayerNorm and BatchNorm rely on computing the mean and variance of activations across dimensions
      if "norm" in name:
        module = module.to(torch.float32)

      # lm_head is the final output layer in an LLM. It is a linear layer that maps hidden
      # states from the last transformer layer to vocabulary logits.
      if "lm_head" in name or "embed_tokens" in name:
        if hasattr(module, "weight"):
          if bf16 and module.weight.dtype == torch.float32:
            module = module.to(torch.bfloat16)

  model.print_trainable_parameters()

  return model

def training_function(script_args, training_args):
    # load dataset
    dataset = load_from_disk(script_args.dataset_path)

    # bitesandbytes config
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )

    model = AutoModelForCausalLM.from_pretrained(
        script_args.model_id,
        use_cache=False if training_args.gradient_checkpointing else True, # this is needed for gradient checkpointing,
        device_map="auto",
        use_flash_attention_2=script_args.use_flash_attn,
        quantization_config=bnb_config
    )

    # peft config
    model = create_peft_model(
        model,
        script_args.gradient_checkpointing,
        script_args.bf16
    )

    # create trainer instance
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset = dataset,
        data_collator=default_data_collator # no special collator needed since we stacked the dataset
    )

    trainer.train()

    sagemaker_save_dir = "/opt/ml/model/"
    if script_args.merge_adapters:
        # merge adapter weights with base model and save
        # save int 4 model

        # safe_serialization=False
        # Saves only the LoRA adapter weights (without merging with the base model).
        # Keeps flexibility: allows you to later merge adapters into any compatible base model.
        # Uses PyTorch .bin for compatibility.
        trainer.model.save_pretrained(training_args.output_dir, safe_serialization=False)

        # clear memory
        del model
        del trainer
        torch.cuda.empty_cache()

        # load PEFT model in fp16
        model = AutoPeftModelForCausalLM.from_pretrained(
            training_args.output_dir,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float16,
        )

        # safe_serialization=True
        # Merges LoRA with the base model to create a single, fully fine-tuned model.
        # Saves using SafeTensors (.safetensors), which is more efficient and safer than .bin.
        # Ensures correct format for deployment and prevents potential security risks.
        model = model.merge_and_unload()

        # Mistral 7B and similar models are dozens of GBs in size. A fully merged model can exceed 32GB in float16 or bfloat16.
        # Instead of saving one giant model file, sharding splits the model into multiple 2GB chunks.
        # When loading, the model automatically reconstructs itself from these shards.
        model.save_pretrained(sagemaker_save_dir, safe_serialization=True, max_shard_size="2GB")
    else:
      model.save_pretrained(sagemaker_save_dir, safe_serialization=True)

    # save tokenizer for easy inference
    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, padding_side='left')
    tokenizer.save_pretrained(sagemaker_save_dir)

from dataclasses import dataclass, field

@dataclass
class ScriptArgiments:
    model_id: str = field(
        metadata={"help":"The model that you wnat to experiment on from huggingface. For example gpt2, llama"}
    )

    dataset_path: str = field(
        metadata={"help":"where to save the preprocessed data for the training"}, default=None,
    )

    hf_token: Optional[str] = field(default=None, metadata={"help":"hugging face token to access the models repository"})

    use_flash_attn: bool = field(default=False, metadata={"help":"Whether to use flash attention or not"})

    merge_adapters: bool = field(default=False, metadata={"help":"Whether to merge the LoRA and base model weights"})


def main():
    arg_parser = HfArgumentParser([ScriptArgiments, TrainingArguments])
    script_args, training_args = arg_parser.parse_args_into_dataclasses()

    set_seed(training_args.seed)

    token = script_args.hf_token if script_args.hf_token else os.getenv("HF_TOKEN", None)
    if token:
      login(token=token)

    training_function(script_args, training_args)

if __name__ == "__main__":
    main()

